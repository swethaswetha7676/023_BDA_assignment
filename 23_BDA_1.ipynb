{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPER5WQ3l93V4QbjUTbk6wz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swethaswetha7676/023_BDA_assignment/blob/main/23_BDA_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build *italicized text* a classification model with spark with a dataset of your choice**"
      ],
      "metadata": {
        "id": "Iz_op1LY5Y54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description:\n",
        "1. Environment Preparation: The code starts by importing essential libraries, including pyspark for distributed data processing, pandas for data manipulation, and others for machine learning tasks and model evaluation. It then establishes a SparkSession, which acts as the primary entry point for interacting with Spark functionalities and provides a structured environment for executing Spark applications.\n",
        "\n",
        "2. Data Acquisition and Preprocessing: The code proceeds to download a wine quality dataset from a publicly available source and loads it into a Spark DataFrame, a distributed collection of data organized into named columns. To facilitate classification, a new column named \"label\" is created, assigning a value of 1 to wines deemed \"good\" based on a quality threshold and 0 to those considered \"not good.\" Relevant features, representing characteristics of the wine, are selected alongside the target variable (\"label\"). These features are then transformed into a vector representation using VectorAssembler, a crucial step for compatibility with many machine learning algorithms.\n",
        "\n",
        "3. Model Training and Development: The dataset is divided into two subsets: a training set used to train the model and a testing set used to evaluate its performance on unseen data. Logistic Regression, a widely used algorithm for binary classification, is chosen as the model for predicting wine quality. The model is trained using the training data, allowing it to learn patterns and relationships between the features and the target variable (wine quality).\n",
        "\n",
        "4. Prediction and Performance Assessment: The trained Logistic Regression model is applied to the testing data to predict the quality labels for each wine. The accuracy of these predictions is then evaluated using metrics such as Area Under the ROC Curve (AUC) and overall accuracy. These metrics provide insights into the model's ability to generalize to new data and its effectiveness in correctly classifying wine quality.\n",
        "\n",
        "5. Resource Management and Termination: To ensure efficient resource utilization, the SparkSession is terminated, releasing the resources allocated during the analysis. This step is crucial for maintaining a clean and optimized execution environment."
      ],
      "metadata": {
        "id": "NbGM2FC8fcKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"WineQualityClassification\").getOrCreate()"
      ],
      "metadata": {
        "id": "oZwFolNz3Y1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "data_file = \"winequality-red.csv\"\n",
        "if not os.path.exists(data_file):\n",
        "    !pip install wget\n",
        "    import wget\n",
        "    wget.download(data_url, data_file)\n",
        "df = spark.read.csv(data_file, sep=\";\", header=True, inferSchema=True)\n",
        "df.show(5)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGdRSNvT3bXP",
        "outputId": "03df626f-bd21-4038-a182-8e4899d1c056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=d69d1b9f2930573a5e8e6c957a1c0a79d4626c8ab0d5bc77040dceac358383a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
            "|fixed acidity|volatile acidity|citric acid|residual sugar|chlorides|free sulfur dioxide|total sulfur dioxide|density|  pH|sulphates|alcohol|quality|\n",
            "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
            "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|\n",
            "|          7.8|            0.88|        0.0|           2.6|    0.098|               25.0|                67.0| 0.9968| 3.2|     0.68|    9.8|      5|\n",
            "|          7.8|            0.76|       0.04|           2.3|    0.092|               15.0|                54.0|  0.997|3.26|     0.65|    9.8|      5|\n",
            "|         11.2|            0.28|       0.56|           1.9|    0.075|               17.0|                60.0|  0.998|3.16|     0.58|    9.8|      6|\n",
            "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|\n",
            "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- fixed acidity: double (nullable = true)\n",
            " |-- volatile acidity: double (nullable = true)\n",
            " |-- citric acid: double (nullable = true)\n",
            " |-- residual sugar: double (nullable = true)\n",
            " |-- chlorides: double (nullable = true)\n",
            " |-- free sulfur dioxide: double (nullable = true)\n",
            " |-- total sulfur dioxide: double (nullable = true)\n",
            " |-- density: double (nullable = true)\n",
            " |-- pH: double (nullable = true)\n",
            " |-- sulphates: double (nullable = true)\n",
            " |-- alcohol: double (nullable = true)\n",
            " |-- quality: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "# Create a binary label: 1 for good quality (>= 7), 0 for not good\n",
        "df = df.withColumn(\"label\", when(df[\"quality\"] >= 7, 1.0).otherwise(0.0))\n",
        "# Select the features and the new label column\n",
        "feature_columns = [col for col in df.columns if col != \"quality\" and col != \"label\"]\n",
        "data = df.select(feature_columns + [\"label\"])\n",
        "data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SWsZdHi4BU3",
        "outputId": "7759e113-6992-44fa-c606-da38c2168265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-----+\n",
            "|fixed acidity|volatile acidity|citric acid|residual sugar|chlorides|free sulfur dioxide|total sulfur dioxide|density|  pH|sulphates|alcohol|label|\n",
            "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-----+\n",
            "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|  0.0|\n",
            "|          7.8|            0.88|        0.0|           2.6|    0.098|               25.0|                67.0| 0.9968| 3.2|     0.68|    9.8|  0.0|\n",
            "|          7.8|            0.76|       0.04|           2.3|    0.092|               15.0|                54.0|  0.997|3.26|     0.65|    9.8|  0.0|\n",
            "|         11.2|            0.28|       0.56|           1.9|    0.075|               17.0|                60.0|  0.998|3.16|     0.58|    9.8|  0.0|\n",
            "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|  0.0|\n",
            "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "feature_assembler = VectorAssembler(            # Vectorize the feature columns\n",
        "    inputCols=feature_columns,\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "output = feature_assembler.transform(data)\n",
        "final_df = output.select(\"features\", \"label\")   # Select the features and label\n",
        "final_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcYN_UrK4KF_",
        "outputId": "7f51ba08-a331-473c-f558-5e29bebc0893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|[7.4,0.7,0.0,1.9,...|  0.0|\n",
            "|[7.8,0.88,0.0,2.6...|  0.0|\n",
            "|[7.8,0.76,0.04,2....|  0.0|\n",
            "|[11.2,0.28,0.56,1...|  0.0|\n",
            "|[7.4,0.7,0.0,1.9,...|  0.0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)\n",
        "print(f\"Number of training samples: {train_data.count()}\")\n",
        "print(f\"Number of testing samples: {test_data.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McCplUnA4X1n",
        "outputId": "c0fa69a1-25a0-4156-f05b-357ad78da67d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 1324\n",
            "Number of testing samples: 275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "model = lr.fit(train_data)\n",
        "predictions = model.transform(test_data)              # Make predictions on the test data\n",
        "predictions.select(\"features\", \"label\", \"prediction\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey5Mb2AF4r_f",
        "outputId": "1d5843e3-38c3-426d-a259-5ad6e32e7d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+\n",
            "|            features|label|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|[4.9,0.42,0.0,2.1...|  1.0|       1.0|\n",
            "|[5.0,0.74,0.0,1.2...|  0.0|       0.0|\n",
            "|[5.0,1.04,0.24,1....|  0.0|       0.0|\n",
            "|[5.2,0.32,0.25,1....|  0.0|       0.0|\n",
            "|[5.3,0.47,0.11,2....|  1.0|       1.0|\n",
            "|[5.4,0.42,0.27,2....|  1.0|       0.0|\n",
            "|[5.6,0.31,0.37,1....|  0.0|       0.0|\n",
            "|[5.6,0.605,0.05,2...|  0.0|       0.0|\n",
            "|[5.8,0.29,0.26,1....|  0.0|       1.0|\n",
            "|[5.8,0.61,0.11,1....|  0.0|       0.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)       # Calculate the Area Under ROC Curve (AUC)\n",
        "print(f\"Area Under ROC Curve (AUC) on the test data: {auc:.4f}\")\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = accuracy_evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy of the model on the test data: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHJcko234vYW",
        "outputId": "41d3e9fd-6ef3-45d8-aa35-39f5b6cd7c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC Curve (AUC) on the test data: 0.8623\n",
            "Accuracy of the model on the test data: 0.8727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "icu2FMvW5A03"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}